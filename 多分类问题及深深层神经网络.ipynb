{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST手写体识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MNIST`手写体识别是图像识别中最经典的问题, 希望能够识别出人类手写的数字. 数据是65000张灰度图和对应的数字. 我们用之前的深度神经网络来尝试解决这个问题."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Tensorflow 已经把 mnist 数据集集成在 examples 里面了\n",
    "# 在这里 import 数据输入的部分\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "tf.set_random_seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们导入`mnist`数据集\n",
    "\n",
    "**注意** 下载数据需要一定时间, 如果下面这行代码下载出现问题, 可以从网上下载 MNIST 数据集, 然后一起放到`MNIST_data`这个文件夹中, 文件夹的结构应该是下面这样:\n",
    "```\n",
    "    MNIST_data\n",
    "        train-images-idx3-ubyte.gz\n",
    "        train-labels-idx1-ubyte.gz\n",
    "        t10k-images-idx3-ubyte.gz\n",
    "        t10k-labels-idx1-ubyte.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到`read_data_sets`的一个参数是**`one_hot`**([wiki](https://en.wikipedia.org/wiki/One-hot)). \n",
    "\n",
    "它是识别任务中非常重要的一个概念, 将一个数值`n`映射到一个向量, 这个向量的第`n`个元素是`1`, 其他元素都是`0`.\n",
    "\n",
    "这个数据集分成了两个部分: 训练和测试. 分开来是为了观察模型在完全没见过的数据上的表现, 从而体现泛化能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist.train\n",
    "test_set = mnist.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们具体看看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(ncols=6, nrows=2)\n",
    "plt.tight_layout(w_pad=-2.0, h_pad=-8.0)\n",
    "\n",
    "# 调用next_batch方法来一次性获取12个样本,\n",
    "# 这里有一个`shuffle`参数, 表达是否打乱样本间的顺序\n",
    "images, labels = train_set.next_batch(12, shuffle=False)\n",
    "\n",
    "for ind, (image, label) in enumerate(zip(images, labels)):\n",
    "    # image 是一个 784 维的向量, 是图片进行拉伸产生的, 这里我们给它 reshape 回去\n",
    "    image = image.reshape((28, 28))\n",
    "    \n",
    "    # label 是一个 10 维的向量, 哪个下标处的值为1 说明是数字几\n",
    "    label = label.argmax()\n",
    "\n",
    "    row = ind // 6\n",
    "    col = ind % 6\n",
    "    axes[row][col].imshow(image, cmap='gray') # 灰度图\n",
    "    axes[row][col].axis('off')\n",
    "    axes[row][col].set_title('%d' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来定义深度网络结构."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer(layer_input, output_depth, scope='hidden_layer', reuse=None):\n",
    "    input_depth = layer_input.get_shape()[-1]\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 注意这里的初始化方法是truncated_normal\n",
    "        w = tf.get_variable(initializer=tf.truncated_normal_initializer(stddev=0.1), shape=(input_depth, output_depth), name='weights')\n",
    "        # 注意这里用 0.1 对偏置进行初始化\n",
    "        b = tf.get_variable(initializer=tf.constant_initializer(0.1), shape=(output_depth), name='bias')\n",
    "        net = tf.matmul(layer_input, w) + b\n",
    "        \n",
    "        return net\n",
    "\n",
    "def DNN(x, output_depths, scope='DNN', reuse=None):\n",
    "    net = x\n",
    "    for i, output_depth in enumerate(output_depths):\n",
    "        net = hidden_layer(net, output_depth, scope='layer%d' % i, reuse=reuse)\n",
    "        # 注意这里的激活函数\n",
    "        net = tf.nn.relu(net)\n",
    "    # 数字分为0, 1, ..., 9 所以这是10分类问题\n",
    "    # 对应于 one_hot 的标签, 所以这里输出一个 10维 的向量\n",
    "    net = hidden_layer(net, 10, scope='classification', reuse=reuse)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义占位符\n",
    "input_ph = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None, 10), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造一个4层的神经网络, 它的隐藏节点数分别为: 400, 200, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = DNN(input_ph, [400, 200, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From h:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# 这是一个分类问题, 因此我们采用交叉熵来计算损失函数\n",
    "loss = tf.losses.softmax_cross_entropy(logits=dnn, onehot_labels=label_ph)\n",
    "\n",
    "# 下面定义的是正确率, 注意理解它为什么是这么定义的\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn, axis=-1), tf.argmax(label_ph, axis=-1)), dtype=tf.float32))\n",
    "\n",
    "lr = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1000: train_loss: 0.508145 train_acc: 0.859375 test_loss: 0.381747 test_acc: 0.890625\n",
      "STEP 2000: train_loss: 0.273148 train_acc: 0.906250 test_loss: 0.308214 test_acc: 0.937500\n",
      "STEP 3000: train_loss: 0.300777 train_acc: 0.921875 test_loss: 0.129173 test_acc: 0.968750\n",
      "STEP 4000: train_loss: 0.287420 train_acc: 0.906250 test_loss: 0.137719 test_acc: 0.953125\n",
      "STEP 5000: train_loss: 0.158032 train_acc: 0.968750 test_loss: 0.342953 test_acc: 0.921875\n",
      "STEP 6000: train_loss: 0.049226 train_acc: 0.984375 test_loss: 0.233301 test_acc: 0.937500\n",
      "STEP 7000: train_loss: 0.147544 train_acc: 0.953125 test_loss: 0.146067 test_acc: 0.937500\n",
      "STEP 8000: train_loss: 0.167651 train_acc: 0.953125 test_loss: 0.195560 test_acc: 0.921875\n",
      "STEP 9000: train_loss: 0.077194 train_acc: 0.968750 test_loss: 0.126170 test_acc: 0.968750\n",
      "STEP 10000: train_loss: 0.121299 train_acc: 0.984375 test_loss: 0.106234 test_acc: 0.937500\n",
      "STEP 11000: train_loss: 0.079348 train_acc: 0.984375 test_loss: 0.070369 test_acc: 0.968750\n",
      "STEP 12000: train_loss: 0.136509 train_acc: 0.968750 test_loss: 0.040164 test_acc: 1.000000\n",
      "STEP 13000: train_loss: 0.082930 train_acc: 0.953125 test_loss: 0.167785 test_acc: 0.953125\n",
      "STEP 14000: train_loss: 0.046076 train_acc: 0.984375 test_loss: 0.252459 test_acc: 0.921875\n",
      "STEP 15000: train_loss: 0.054716 train_acc: 0.984375 test_loss: 0.075712 test_acc: 0.968750\n",
      "STEP 16000: train_loss: 0.060598 train_acc: 0.968750 test_loss: 0.062007 test_acc: 0.968750\n",
      "STEP 17000: train_loss: 0.015373 train_acc: 1.000000 test_loss: 0.160953 test_acc: 0.937500\n",
      "STEP 18000: train_loss: 0.004166 train_acc: 1.000000 test_loss: 0.124242 test_acc: 0.968750\n",
      "STEP 19000: train_loss: 0.044605 train_acc: 0.984375 test_loss: 0.125385 test_acc: 0.968750\n",
      "STEP 20000: train_loss: 0.033307 train_acc: 1.000000 test_loss: 0.160183 test_acc: 0.953125\n",
      "Train Done!\n",
      "------------------------------\n",
      "Train loss: 0.049389\n",
      "Train accuracy: 0.986982\n",
      "Test loss: 0.090362\n",
      "Test accuracy: 0.970800\n"
     ]
    }
   ],
   "source": [
    "# 我们训练20000次\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(20000):\n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(batch_size)\n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    if e % 1000 == 999:\n",
    "        # 获取 batch_size 个测试样本\n",
    "        test_imgs, test_labels = test_set.next_batch(batch_size)\n",
    "        # 计算在当前样本上的训练以及测试样本的损失值和正确率\n",
    "        loss_train, acc_train = sess.run([loss, acc], feed_dict={input_ph: images, label_ph: labels})\n",
    "        loss_test, acc_test = sess.run([loss, acc], feed_dict={input_ph: test_imgs, label_ph: test_labels})\n",
    "        print('STEP {}: train_loss: {:.6f} train_acc: {:.6f} test_loss: {:.6f} test_acc: {:.6f}'.format(e + 1, loss_train, acc_train, loss_test, acc_test))\n",
    "\n",
    "print('Train Done!')\n",
    "print('-'*30)\n",
    "\n",
    "# 计算所有训练样本的损失值以及正确率\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "for _ in range(train_set.num_examples // 100):\n",
    "    image, label = train_set.next_batch(100)\n",
    "    loss_train, acc_train = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    train_loss.append(loss_train)\n",
    "    train_acc.append(acc_train)\n",
    "\n",
    "print('Train loss: {:.6f}'.format(np.array(train_loss).mean()))\n",
    "print('Train accuracy: {:.6f}'.format(np.array(train_acc).mean()))\n",
    "\n",
    "# 计算所有测试样本的损失值以及正确率\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "for _ in range(test_set.num_examples // 100):\n",
    "    image, label = test_set.next_batch(100)\n",
    "    loss_test, acc_test = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    test_loss.append(loss_test)\n",
    "    test_acc.append(acc_test)\n",
    "\n",
    "print('Test loss: {:.6f}'.format(np.array(test_loss).mean()))\n",
    "print('Test accuracy: {:.6f}'.format(np.array(test_acc).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 最后在训练集上我们达到了大约0.98的正确率, 在测试集上我们达到了大约0.97的正确率, 已经是一个相当不错的成绩了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard & `tf.summary`\n",
    "\n",
    "[前面](https://github.com/SherlockLiao/ai-class-intro/blob/master/tensorflow/course_0/tensorflow-basic.ipynb)已经给大家分享过如何使用`Tensorboard`将我们构建的计算图显示出来, 这里我们还要介绍它和`tf.summary`结合起来体现的更加强大的功能: 可视化训练. \n",
    "\n",
    "首先介绍一下`tf.summary`, 它能够收集训练过程中的各种`tensor`的信息并把它保存起来以供`Tensorboard`读取并展示. 按照下面的方法来使用它:\n",
    "\n",
    "### 构造`summary`\n",
    "- - -\n",
    "- 如果你想收集表示一个标量或者一个数的`tensor`的信息, 比如上面的`loss`\n",
    "```python\n",
    "loss_sum = tf.summary.scalar('loss', loss)\n",
    "```\n",
    "上面的语句就会告诉`Tensorflow`, 在运行过程中, 我要让`Tensorboard`显示误差的变化了\n",
    "- - -\n",
    "- 如果你想收集一个`tensor`的分布情况, 这个`tensor`可以是任意形状, 比如我们定义了一个`(784, 400)`的权重`w`\n",
    "```python\n",
    "w_hist = tf.summary.histogram('w_hist', w)\n",
    "```\n",
    "- - -\n",
    "- 如果你想收集一个4维的1-通道(灰度图), 3-通道(RGB), 4-通道(RGBA)的`tensor`的变化, 比如我们输出了一个`(1, 8, 8, 1)`的灰度图`image`\n",
    "```python\n",
    "image_sum = tf.summary.image('image', image)\n",
    "```\n",
    "- - -\n",
    "- 如果你想收集一个3维(batch, frame, channel), 2维(batch, frame)的变化, 比如我们输出了一个`(10, 50, 3)`的`tensor`:`audio`\n",
    "```python\n",
    "audio_sum = tf.summary.audio('audio', audio)\n",
    "```\n",
    "- - -\n",
    "在这次课程中, 我们暂时先使用`scalar`和`histogram`的`summary`, `image`和`audio`的`summary`将在之后的课程中介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置计算图\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 重新定义占位符\n",
    "input_ph = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None, 10), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在, 我们需要重新构建前向神经网络, 为了简化代码, 我们在构造一个隐藏层以及它的参数的函数内部构造`tf.summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造权重, 用`truncated_normal`初始化\n",
    "def weight_variable(shape):\n",
    "    init = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "# 构造偏置, 用`0.1`初始化\n",
    "def bias_variable(shape):\n",
    "    init = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造添加`variable`的`summary`的函数\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        # 计算平均值\n",
    "        mean = tf.reduce_mean(var)\n",
    "        # 将平均值添加到`summary`中, 这是一个数值, 所以我们用`tf.summary.scalar`\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        \n",
    "        # 计算标准差\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        # 将标准差添加到`summary`中\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        \n",
    "        # 添加最大值,最小值`summary`\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        \n",
    "        # 添加这个变量分布情况的`summary`, 我们希望观察它的分布, 所以用`tf.summary.histogram`\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个隐藏层\n",
    "def hidden_layer(x, output_dim, scope='hidden_layer', act = tf.nn.relu, reuse=None):\n",
    "    # 获取输入的`depth`\n",
    "    input_dim = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.name_scope(scope):\n",
    "        with tf.name_scope('weight'):\n",
    "            # 构造`weight`\n",
    "            weight = weight_variable([input_dim, output_dim])\n",
    "            # 添加`weight`的`summary`\n",
    "            variable_summaries(weight)\n",
    "            \n",
    "        with tf.name_scope('bias'):\n",
    "            # 构造`bias`\n",
    "            bias = bias_variable([output_dim])\n",
    "            # 添加`bias`的`summary`\n",
    "            variable_summaries(bias)\n",
    "            \n",
    "        with tf.name_scope('linear'):\n",
    "            # 计算`xw+b`\n",
    "            preact = tf.matmul(x, weight) + bias\n",
    "            # 添加激活层之前输出的分布情况到`summary`\n",
    "            tf.summary.histogram('pre_activation', preact)\n",
    "            \n",
    "        # 经过激活层`act`\n",
    "        output = act(preact)\n",
    "        # 添加激活后输出的分布情况到`summary`\n",
    "        tf.summary.histogram('output', output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造深度神经网络\n",
    "def DNN(x, output_depths, scope='DNN_with_sums', reuse=None):\n",
    "    with tf.name_scope(scope):\n",
    "        net = x\n",
    "        for i, output_depth in enumerate(output_depths):\n",
    "            net = hidden_layer(net, output_depth, scope='hidden%d' % (i + 1), reuse=reuse)\n",
    "        # 最后有一个分类层\n",
    "        net = hidden_layer(net, 10, scope='classification', act=tf.identity, reuse=reuse)\n",
    "        return net\n",
    "\n",
    "dnn_with_sums = DNN(input_ph, [400, 200, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新定义`loss`, `acc`, `train_op`\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    loss = tf.losses.softmax_cross_entropy(logits=dnn_with_sums, onehot_labels=label_ph)\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn_with_sums, axis=-1), tf.argmax(label_ph, axis=-1)), dtype=tf.float32))\n",
    "    tf.summary.scalar('accuracy', acc)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    lr = 0.01\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (可选)融合`summary`\n",
    "- - -\n",
    "- 我们可以把前面定义的所有`summary`都融合成一个`summary`\n",
    "```python\n",
    "merged = tf.summary.merge_all()\n",
    "```\n",
    "- - -\n",
    "- 也可以只是融合某些`summary`\n",
    "```python\n",
    "merged = tf.summary.merge([loss_sum, image_sum])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出`summary`\n",
    "---\n",
    "`summary`是需要导出到外部文件的\n",
    "- 首先定义一个文件读写器\n",
    "```python\n",
    "summary_writer = tf.summary.FileWriter('summaries', sess.graph)\n",
    "```\n",
    "- - -\n",
    "- 然后在训练的过程中, 在你希望的时候运行一次`merged`或者是你之前自己定义的某个通过`summary`定义的`op`\n",
    "```python\n",
    "summaries = sess.run(merged, feed_dict={...})\n",
    "```\n",
    "- - -\n",
    "- 然后将这个`summaries`写入到`summari_writer`内\n",
    "```python\n",
    "summary_writer.add_summary(summaries, step)\n",
    "```\n",
    "注意`step`表示你当前训练的步数, 当然你也可以设定为其他你想要用的数值\n",
    "\n",
    "- - -\n",
    "- 最后关闭文件读写器\n",
    "```python\n",
    "summary_writer.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('test_summary/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter('test_summary/test', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1000: train_loss: 0.420341 train_acc: 0.906250 test_loss: 0.349639 test_acc: 0.921875\n",
      "STEP 2000: train_loss: 0.507827 train_acc: 0.875000 test_loss: 0.419028 test_acc: 0.859375\n",
      "STEP 3000: train_loss: 0.230171 train_acc: 0.953125 test_loss: 0.274920 test_acc: 0.906250\n",
      "STEP 4000: train_loss: 0.052454 train_acc: 0.984375 test_loss: 0.098679 test_acc: 0.984375\n",
      "STEP 5000: train_loss: 0.084307 train_acc: 1.000000 test_loss: 0.084754 test_acc: 0.968750\n",
      "STEP 6000: train_loss: 0.187624 train_acc: 0.968750 test_loss: 0.045327 test_acc: 1.000000\n",
      "STEP 7000: train_loss: 0.024668 train_acc: 1.000000 test_loss: 0.123194 test_acc: 0.968750\n",
      "STEP 8000: train_loss: 0.076065 train_acc: 0.968750 test_loss: 0.067626 test_acc: 0.984375\n",
      "STEP 9000: train_loss: 0.281054 train_acc: 0.937500 test_loss: 0.205866 test_acc: 0.953125\n",
      "STEP 10000: train_loss: 0.018019 train_acc: 1.000000 test_loss: 0.122319 test_acc: 0.968750\n",
      "STEP 11000: train_loss: 0.082194 train_acc: 0.984375 test_loss: 0.085551 test_acc: 0.968750\n",
      "STEP 12000: train_loss: 0.089316 train_acc: 0.968750 test_loss: 0.187062 test_acc: 0.953125\n",
      "STEP 13000: train_loss: 0.096282 train_acc: 0.984375 test_loss: 0.044740 test_acc: 0.984375\n",
      "STEP 14000: train_loss: 0.038268 train_acc: 0.984375 test_loss: 0.068988 test_acc: 0.968750\n",
      "STEP 15000: train_loss: 0.035274 train_acc: 1.000000 test_loss: 0.137072 test_acc: 0.968750\n",
      "STEP 16000: train_loss: 0.081355 train_acc: 0.953125 test_loss: 0.049799 test_acc: 1.000000\n",
      "STEP 17000: train_loss: 0.038217 train_acc: 1.000000 test_loss: 0.023465 test_acc: 1.000000\n",
      "STEP 18000: train_loss: 0.011341 train_acc: 1.000000 test_loss: 0.170449 test_acc: 0.968750\n",
      "STEP 19000: train_loss: 0.010564 train_acc: 1.000000 test_loss: 0.202906 test_acc: 0.953125\n",
      "STEP 20000: train_loss: 0.048450 train_acc: 0.984375 test_loss: 0.052052 test_acc: 0.968750\n",
      "Train Done!\n",
      "------------------------------\n",
      "Train loss: 0.057317\n",
      "Train accuracy: 0.982855\n",
      "Test loss: 0.102843\n",
      "Test accuracy: 0.969500\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(20000):\n",
    "    images, labels = train_set.next_batch(batch_size)\n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    if e % 1000 == 999:\n",
    "        test_imgs, test_labels = test_set.next_batch(batch_size)\n",
    "        # 获取`train`数据的`summaries`以及`loss`, `acc`信息\n",
    "        sum_train, loss_train, acc_train = sess.run([merged, loss, acc], feed_dict={input_ph: images, label_ph: labels})\n",
    "        # 将`train`的`summaries`写入到`train_writer`中\n",
    "        train_writer.add_summary(sum_train, e)\n",
    "        # 获取`test`数据的`summaries`以及`loss`, `acc`信息\n",
    "        sum_test, loss_test, acc_test = sess.run([merged, loss, acc], feed_dict={input_ph: test_imgs, label_ph: test_labels})\n",
    "        # 将`test`的`summaries`写入到`test_writer`中\n",
    "        test_writer.add_summary(sum_test, e)\n",
    "        print('STEP {}: train_loss: {:.6f} train_acc: {:.6f} test_loss: {:.6f} test_acc: {:.6f}'.format(e + 1, loss_train, acc_train, loss_test, acc_test))\n",
    "\n",
    "# 关闭读写器\n",
    "train_writer.close()\n",
    "test_writer.close()\n",
    "\n",
    "print('Train Done!')\n",
    "print('-'*30)\n",
    "\n",
    "# 计算所有训练样本的损失值以及正确率\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "for _ in range(train_set.num_examples // 100):\n",
    "    image, label = train_set.next_batch(100)\n",
    "    loss_train, acc_train = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    train_loss.append(loss_train)\n",
    "    train_acc.append(acc_train)\n",
    "\n",
    "print('Train loss: {:.6f}'.format(np.array(train_loss).mean()))\n",
    "print('Train accuracy: {:.6f}'.format(np.array(train_acc).mean()))\n",
    "\n",
    "# 计算所有测试样本的损失值以及正确率\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "for _ in range(test_set.num_examples // 100):\n",
    "    image, label = test_set.next_batch(100)\n",
    "    loss_test, acc_test = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    test_loss.append(loss_test)\n",
    "    test_acc.append(acc_test)\n",
    "\n",
    "print('Test loss: {:.6f}'.format(np.array(test_loss).mean()))\n",
    "print('Test accuracy: {:.6f}'.format(np.array(test_acc).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开`Tensorboard`\n",
    "在之前对计算图可视化的时候, 我们用`tensorboard --logdir=.`命令打开过`Tensorboard`显示当前目录下``. 但`Tensorboard`支持打开多个目录下的`.events`文件, 方便我们对比不同模型或者训练和测试之间的差别\n",
    "\n",
    "\n",
    "在`test_summary`目录中输入以下命令\n",
    "```\n",
    "$ tensorboard --logdir=train:train/,test:test/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们就可以看到类似于下面的界面了\n",
    "- **计算图**\n",
    "<img src=\"https://image.ibb.co/fGfMSc/graph.png\" width=\"500\">\n",
    "- - -\n",
    "- **`loss`和`accuracy`**\n",
    "<figure class=\"half\">\n",
    "    <img src=\"https://image.ibb.co/n1bBSc/loss.png\" align=\"left\">\n",
    "    <img src=\"https://image.ibb.co/dHg0Lx/accuracy.png\" align='right'>\n",
    "</figure>\n",
    "- - -\n",
    "- **所有的标量统计**\n",
    "<img src=\"https://image.ibb.co/kVP9DH/scalars.png\" width=\"800\">\n",
    "- - -\n",
    "- **所有的直方图统计**\n",
    "<img src=\"https://image.ibb.co/bx60Lx/histogram.png\" width=\"800\">\n",
    "- - - \n",
    "- **所有的分布统计**\n",
    "<img src=\"https://image.ibb.co/irS70x/distribution.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensorboard`有各种各样的有意思的地方, 大家可以多探索探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结语\n",
    "这次大家学习到了如何对`MNIST`手写体识别问题构建深度神经网络模型, 以及如何使用`Tensorboard`对训练过程进行可视化. 在接下来的课程中, 我们将要进入到深度学习的卷积神经网络部分"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
